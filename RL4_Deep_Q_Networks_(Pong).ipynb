{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "RL4 - Deep Q-Networks (Pong).ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ZFDt1lvPhA",
        "colab_type": "text"
      },
      "source": [
        "**Class 4: Deep Q Networks.**\n",
        "\n",
        "1. <a href=\"#sec1\">Environments</a>\n",
        "    1. <a href=\"#sec1.1\">Cartpole</a>\n",
        "    2. <a href=\"#sec1.2\">Cartpole swing-up</a>\n",
        "    3. <a href=\"#sec1.3\">Pong</a>\n",
        "2. <a href=\"#sec2\">Value Iteration as a sequence of Supervized Learning problems</a>\n",
        "3. <a href=\"#sec3\">Experience Replay</a>\n",
        "4. <a href=\"#sec4\">A deep Q-network</a>\n",
        "5. <a href=\"#sec5\">Making DQN more efficient</a>\n",
        "    1. <a href=\"#sec5.1\">Changing the optimizer\n",
        "    2. <a href=\"#sec5.2\">Several gradient steps\n",
        "    3. <a href=\"#sec5.3\">Target network</a>\n",
        "    4. <a href=\"#sec5.4\">Error clipping</a>\n",
        "6. <a href=\"#sec6\">Metrics</a>\n",
        "7. <a href=\"#sec7\">DQN on image-based tasks</a>\n",
        "8. <a href=\"#sec8\">Going further</a>\n",
        "\n",
        "Let's start with this quote:\n",
        "\n",
        "> The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence. (Richard S. Sutton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki0BrEgNvPhH",
        "colab_type": "text"
      },
      "source": [
        "# <a id=\"sec1\"></a> Environments\n",
        "\n",
        "In this session, we will work with three different environments:\n",
        "- CartPole\n",
        "- A modified version of CartPole\n",
        "- Pong"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr6JF31QvPhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "logger.set_level(gym.logger.DISABLED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhk_b20UvPim",
        "colab_type": "text"
      },
      "source": [
        "## <a id=sec1.3></a>Pong\n",
        "\n",
        "Let's build an agent that learns to play Pong, one of the [Atari games](https://github.com/openai/gym/blob/master/gym/envs/atari/atari_env.py) in Gym (originally in the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)). You might want to try different games later on (like the popular Breakout game for instance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph0x94JLvPip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pong = gym.make('Pong-v4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l095qOuTvPiu",
        "colab_type": "text"
      },
      "source": [
        "Here is the environment's description.\n",
        "> Maximize your score in the Atari 2600 game Pong. In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3). Each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from $\\{2, 3, 4\\}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5nE7HxvPiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "fc90695a-ccbd-4096-b90c-0233b02255fb"
      },
      "source": [
        "print(pong.observation_space)\n",
        "print(pong.observation_space.shape)\n",
        "print(np.min(pong.observation_space.low))\n",
        "print(np.max(pong.observation_space.high))\n",
        "print(pong.action_space)\n",
        "#help(env.observation_space)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(210, 160, 3)\n",
            "(210, 160, 3)\n",
            "0\n",
            "255\n",
            "Discrete(6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCHiwgHNvPi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "5d8f4a4c-73e0-4454-d026-316e06c860fe"
      },
      "source": [
        "x = pong.reset()\n",
        "plt.imshow(x)\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCk82-r1vPi8",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "\n",
        "**Exercice:** What is the number of possible states? Why is this not an MDP? What would one need to turn this back into an MDP?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjhfBOFUvPi-",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-danger\">\n",
        "    <a href=\"#Pong-theory\" data-toggle=\"collapse\"><b>Answers:</b></a><br>\n",
        "<div id=\"Pong-theory\" class=\"collapse\">\n",
        "\n",
        "One frame is a $210\\times 160$ RGB image with a 256 color palette, so the set of all possible frames has size $256^{210 \\times 160 \\times 3} \\sim 10^{242579}$. That's a little too many for an efficient enumeration. Of course, most of the possible images will never occur in a Breakout game and the true state space is actually a much smaller subset of the full set of possible images. Nevertheless, unless we provide a large engineering effort in describing the state space with few variables (which would be contradictory of our goal of a \"human-level\" AI) we will need to automatically discover some structure in the state sampled data.\n",
        "\n",
        "This is not an MDP because the transition dynamics do not respect Markov's property. The probability of transitioning from $s_t$ to $s_{t+1}$ is *not* independent of previous states. The problem here is that a single frame of the game does not reflect the velocity of the ball.\n",
        "\n",
        "To recover Markov's property one could simply stack a few frames together in the state space.\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcFD0GbxvPjA",
        "colab_type": "text"
      },
      "source": [
        "There are 18 buttons on the Atari controller. However not all games use all buttons. Our interface to Pong specifies 6 possible actions:\n",
        "- 0 NOOP (no operation)\n",
        "- 1 FIRE (press fire button, doesn't do anything in Pong)\n",
        "- 2 RIGHT (actually moves the paddle up in Pong)\n",
        "- 3 LEFT (actually moves the paddle left in Pong)\n",
        "- 4 UP (moves the paddle upwards)\n",
        "- 5 DOWN (moves the paddle downwards)\n",
        "\n",
        "The available actions in Pong go up to the 6th action for naming consistency (UP and DOWN), but the four first actions are not really useful.\n",
        "\n",
        "Also, for an unknown reason, the game does not start until the 20th frame (but always starts automatically, pressing FIRE does not change anything).\n",
        "\n",
        "The frame rate is 60Hz.\n",
        "\n",
        "To avoid confusion between the 6 actions allowed by Gym, let's build a wrapper around our environment, with only 2 possible actions (\"0\" for UP and \"1\" for DOWN) and a downscaled observation space. Unless you're curious and want to dig in the code, you can simply run the following cells and just use the resulting environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TBMVGwjvPjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import AtariPreprocessing\n",
        "import cv2\n",
        "\n",
        "class PongWrapper(AtariPreprocessing):\n",
        "    def __init__(self, env, **kwargs):\n",
        "        super(PongWrapper, self).__init__(env, **kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        return super(PongWrapper, self).step(4 + action)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        if self.frame_skip > 1:  # more efficient in-place pooling\n",
        "            np.maximum(self.obs_buffer[0], self.obs_buffer[1], out=self.obs_buffer[0])\n",
        "        obs = cv2.resize(self.obs_buffer[0], (84, 110), interpolation=cv2.INTER_AREA)[17:101,:]\n",
        "\n",
        "        if self.scale_obs:\n",
        "            obs = np.asarray(obs, dtype=np.float32) / 255.0\n",
        "        else:\n",
        "            obs = np.asarray(obs, dtype=np.uint8)\n",
        "        return obs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKsNw7v9vPjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pong = PongWrapper(gym.make('PongNoFrameskip-v4'),\n",
        "                   noop_max=0,\n",
        "                   frame_skip=4,\n",
        "                   terminal_on_life_loss=True,\n",
        "                   grayscale_obs=True,\n",
        "                   scale_obs=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyS4od8uvPjb",
        "colab_type": "text"
      },
      "source": [
        "# <a id=sec3></a>Experience Replay\n",
        "\n",
        "To recover the independence assumption between samples, we can introduce the mechanism of [*Experience Replay*](http://www.incompleteideas.net/lin-92.pdf) by storing past samples into a *Replay Memory*. When samples a required for a mini-batch gradient update, the samples are collected uniformly from the replay memory, thus mimicking an (almost) independent draw according to $\\rho(\\cdot)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3GqSroevPjc",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    \n",
        "**Exercice:** design a class for the replay memory of the cart-pole example(s). Limit the size of this memory (via a FIFO mechanism) to $10^6$ samples (adapt this number to your computer's RAM). Test it by running a random policy for $2\\cdot 10^6$ time steps.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCuKSvOivPjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR REPLAY BUFFER CODE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaN-xb8jvPkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It will actually be useful to have separate torch.Tensor for the each element type in the sampled minibatch.\n",
        "# That is one Tensor for a minibatch of states, another for actions, etc.\n",
        "# Let's redefine the sample function of our replay buffer class to that end.\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity # capacity of the buffer\n",
        "        self.data = []\n",
        "        self.index = 0 # index of the next cell to be filled\n",
        "\n",
        "    def append(self, s, a, r, s_, d):\n",
        "        if len(self.data) < self.capacity:\n",
        "            self.data.append(None)\n",
        "        self.data[self.index] = (s, a, r, s_, d)\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.data, batch_size)\n",
        "        return list(map(lambda x:torch.Tensor(x).to(device), list(zip(*batch))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prj2zh0BvPkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's define a utility function that gives us the greedy action from a DQN\n",
        "import torch\n",
        "\n",
        "def greedy_action(network, state):\n",
        "    with torch.no_grad():\n",
        "        Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n",
        "        return torch.argmax(Q).detach().cpu().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuvlEJxbvPlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "c57952a7-0161-4637-b0a4-fce40b4542a7"
      },
      "source": [
        "# Let's reset the Q function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "state_dim = cartpole.observation_space.shape[0]\n",
        "n_action = cartpole.action_space.n \n",
        "nb_neurons=24\n",
        "\n",
        "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(nb_neurons, nb_neurons),\n",
        "                          nn.ReLU(), \n",
        "                          nn.Linear(nb_neurons, n_action)).to(device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fb60342489c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstate_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mn_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnb_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cartpole' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ6GhJoJvPlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "\n",
        "class DQN_agent:\n",
        "    def __init__(self, config, model):\n",
        "        self.gamma = config['gamma']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.nb_actions = config['nb_actions']\n",
        "        self.memory = ReplayBuffer(config['buffer_size'])\n",
        "        self.epsilon_max = config['epsilon_max']\n",
        "        self.epsilon_min = config['epsilon_min']\n",
        "        self.epsilon_stop = config['epsilon_stop']\n",
        "        self.epsilon_delay = config['epsilon_delay_decay']\n",
        "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
        "        self.nb_gradient_steps = config['gradient_steps']\n",
        "        self.nb_trials = config['nb_trials'] # NEW NEW NEW\n",
        "        self.total_steps = 0\n",
        "        self.model = model \n",
        "        self.criterion = torch.nn.SmoothL1Loss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['learning_rate'])\n",
        "        self.target_model = deepcopy(self.model).to(device)\n",
        "        self.update_target_freq = config['update_target_freq']\n",
        "\n",
        "    def MC_eval(self, env, nb_trials):  # NEW NEW NEW\n",
        "        MC_total_reward = []\n",
        "        MC_discounted_reward = []\n",
        "        for _ in range(nb_trials):\n",
        "            x = cartpole.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            discounted_reward = 0\n",
        "            step = 0\n",
        "            while not done:\n",
        "                a = greedy_action(self.model, x)\n",
        "                y,r,done,_ = cartpole.step(a)\n",
        "                x = y\n",
        "                total_reward += r\n",
        "                discounted_reward += self.gamma**step * r\n",
        "                step += 1\n",
        "            MC_total_reward.append(total_reward)\n",
        "            MC_discounted_reward.append(discounted_reward)\n",
        "        return np.mean(MC_discounted_reward), np.mean(MC_total_reward)\n",
        "    \n",
        "    def eval_init_state(self, env, nb_trials):   # NEW NEW NEW\n",
        "        with torch.no_grad():\n",
        "            for _ in range(nb_trials):\n",
        "                val = []\n",
        "                x = env.reset()\n",
        "                val.append(self.model(torch.Tensor(x).unsqueeze(0).to(device)).max().item())\n",
        "        return np.mean(val)\n",
        "    \n",
        "    def gradient_step(self):\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
        "            QX = self.model(X)\n",
        "            QY = self.target_model(Y)\n",
        "            QYmax = torch.max(QY, axis=1)[0]\n",
        "            update = QX\n",
        "            A = A.to(torch.long)\n",
        "            update[torch.arange(self.batch_size), A] = torch.addcmul(R, self.gamma, 1-D, QYmax)\n",
        "\n",
        "            loss = self.criterion(self.model(X), update)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step() \n",
        "    \n",
        "    def train(self, env, max_episode):\n",
        "        # Training monitoring\n",
        "        episode_return = []\n",
        "        MC_avg_total_reward = []   # NEW NEW NEW\n",
        "        MC_avg_discounted_reward = []   # NEW NEW NEW\n",
        "        avg_Q_init_state = []   # NEW NEW NEW\n",
        "        \n",
        "        # Initialization\n",
        "        episode = 0\n",
        "        episode_cum_reward = 0\n",
        "        state = env.reset()\n",
        "        epsilon = self.epsilon_max\n",
        "        step = 0\n",
        "        while episode < max_episode:\n",
        "            # update epsilon\n",
        "            if step > self.epsilon_delay:\n",
        "                max(self.epsilon_min, epsilon-self.epsilon_step)\n",
        "\n",
        "            # select epsilon-greedy action\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.randint(self.nb_actions)\n",
        "            else:\n",
        "                action = greedy_action(self.model, state)\n",
        "\n",
        "            # step\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            self.memory.append(state, action, reward, next_state, done)\n",
        "            episode_cum_reward += reward\n",
        "\n",
        "            # train\n",
        "            for _ in range(self.nb_gradient_steps):\n",
        "                self.gradient_step()\n",
        "\n",
        "            # update target network if needed\n",
        "            if step % self.update_target_freq == 0:\n",
        "                self.target_model.load_state_dict(self.model.state_dict())   \n",
        "            \n",
        "            # next transition\n",
        "            step += 1\n",
        "            if done:\n",
        "                episode += 1\n",
        "                \n",
        "                # Monitoring\n",
        "                if self.nb_trials>0:\n",
        "                    MC_dr, MC_tr = self.MC_eval(env, self.nb_trials)    # NEW NEW NEW\n",
        "                    Q0 = self.eval_init_state(env, self.nb_trials)   # NEW NEW NEW\n",
        "                    MC_avg_total_reward.append(MC_tr)   # NEW NEW NEW\n",
        "                    MC_avg_discounted_reward.append(MC_dr)   # NEW NEW NEW\n",
        "                    avg_Q_init_state.append(Q0)   # NEW NEW NEW\n",
        "                    episode_return.append(episode_cum_reward)   # NEW NEW NEW\n",
        "                    print(\"Episode \", '{:2d}'.format(episode), \n",
        "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
        "                          \", batch size \", '{:4d}'.format(len(self.memory)), \n",
        "                          \", ep return \", '{:4.1f}'.format(episode_cum_reward), \n",
        "                          \", MC tot \", '{:6.2f}'.format(MC_tr),\n",
        "                          \", MC disc \", '{:6.2f}'.format(MC_dr),\n",
        "                          \", Q0 \", '{:6.2f}'.format(Q0),\n",
        "                          sep='')\n",
        "                else:\n",
        "                    episode_return.append(episode_cum_reward)\n",
        "                    print(\"Episode \", '{:2d}'.format(episode), \n",
        "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
        "                          \", batch size \", '{:4d}'.format(len(self.memory)), \n",
        "                          \", ep return \", '{:4.1f}'.format(episode_cum_reward), \n",
        "                          sep='')\n",
        "                \n",
        "                # Start new episode\n",
        "                state = env.reset()\n",
        "                episode_cum_reward = 0\n",
        "            else:\n",
        "                state = next_state\n",
        "\n",
        "        return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, avg_Q_init_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR01c38QvPlt",
        "colab_type": "text"
      },
      "source": [
        "# <a id=\"sec7\"></a>DQN on image-based tasks\n",
        "\n",
        "Now it's time to turn towards Pong. As you noted earlier, the frame information in Pong is not sufficient to define an MDP, but stacking several frames together allows to recover the Markov property.\n",
        "\n",
        "we could wish to modify the previous replay buffer so that frames are stored only once (for memory efficiency). Then this new replay buffer would still need to return stacks of 4 frames when `sample()` is called.\n",
        "\n",
        "Fortunately, there's a simpler way to do that.\n",
        "We can use a wrapper (provided by Gym in this case) so that calling env.step(a) returns a stack of 4 frames.\n",
        "This wrapper actually only stores each frame once which optimizes memory efficiency. This way, we can keep on using our previous replay buffer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojPS0HnkvPlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import  FrameStack\n",
        "pong = FrameStack(pong, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSUKJVJvPlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d35731d3-b2ac-40e3-e082-802faf8ebc4f"
      },
      "source": [
        "x = pong.reset()\n",
        "print(torch.Tensor(x).shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 84, 84])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWRRoEGvPlz",
        "colab_type": "text"
      },
      "source": [
        "The two DQN papers ([Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) and [Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)) actually introduce two different neural network architectures.\n",
        "\n",
        "The 2013 paper uses this architecture:\n",
        "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
        "- layer 1: Convolutions with 16 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
        "- layer 2: Convolutions with 32 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
        "- layer 3: Fully connected with 256 ReLU units\n",
        "- layer 4 (output): Fully connected with 2 linear units (one for each action's value)\n",
        "\n",
        "The 2015 paper \n",
        "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
        "- layer 1: Convolutions with 32 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
        "- layer 2: Convolutions with 64 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
        "- layer 3: Convolutions with 64 filters of size $3\\times 3$ and stride 1. The activation is a ReLU function.\n",
        "- layer 4: Fully connected with 512 ReLU units\n",
        "- layer 5 (output): Fully connected with 2 linear units (one for each action's value)\n",
        "\n",
        "Also, it is a good practice to pre-fill the replay buffer with randomly sampled experience. The 2015 paper runs a random policy for 50000 steps to feed the replay buffer before training.\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "    \n",
        "**Exercice:** Create the corresponding neural network and adapt your optimization code from the previous exercice to train on Pong (you can take $C$ much larger, in the order of $10000$).\n",
        "</div>\n",
        "Caveat: unless you have a GPU and a fair amount of time ahead of you, it is recommended to run this computation on a cloud computing service (or on a dediated machine)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBQtYbN5vPl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR DQN CODE FOR PONG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4JOieIrvPl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class AtariCNN(nn.Module):\n",
        "    def __init__(self, in_channels=4, n_actions=2):\n",
        "        super(AtariCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.head = nn.Linear(512, n_actions)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdUjKKJGwvwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2a60462-97cc-4303-dfee-c9cbc25bdce2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHHLX5iuvPl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d89eb42-9aa6-4647-c2fd-80e1063ae7a3"
      },
      "source": [
        "from tqdm import trange\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config = {'observation_space': pong.observation_space.shape[0],\n",
        "          'nb_actions': 2,\n",
        "          'learning_rate': 0.001,\n",
        "          'gamma': 0.95,\n",
        "          'buffer_size': 1000000,\n",
        "          'epsilon_min': 0.1,\n",
        "          'epsilon_max': 1.,\n",
        "          'epsilon_stop': 1000000,\n",
        "          'epsilon_delay_decay': 0,\n",
        "          'batch_size': 32,\n",
        "          'gradient_steps': 10,\n",
        "          'update_target_freq': 10000,\n",
        "          'nb_trials': 0}\n",
        "\n",
        "AtariDQN = AtariCNN().to(device)\n",
        "\n",
        "agent = DQN_agent(config, AtariDQN)\n",
        "\n",
        "# pre-fill the replay buffer\n",
        "x = pong.reset()\n",
        "for t in trange(50000):\n",
        "    a = np.random.randint(2)\n",
        "    y, r, d, _ = pong.step(a)\n",
        "    agent.memory.append(x, a, r, y, d)\n",
        "    if d:\n",
        "        x = pong.reset()\n",
        "    else:\n",
        "        x = y\n",
        "\n",
        "# train\n",
        "ep_length, disc_rewards, tot_rewards, Q0 = agent.train(pong, 30)\n",
        "\n",
        "\n",
        "torch.save(AtariDQN.state_dict(), \"/content/gdrive/pong_dqn.pth\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 44601/50000 [01:01<00:07, 707.01it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv1DFXonvPl9",
        "colab_type": "text"
      },
      "source": [
        "To give you an idea of the behavior of a trained agent, you can check the following videos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaPptN5QvPl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "908187cf-eda1-4ba1-89fe-6bc46214ce43"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo(\"p88R2_3yWPA\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/p88R2_3yWPA\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7fa6442edb38>"
            ],
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAUDBAICAgICAgICAgIGBwIIBwcHBwMCAgkCBgMGBgUD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\nAAAAAAAAP//Z\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF4Vc81ZvPmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "YouTubeVideo(\"TmPfTpjtdgg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILXjOcx8vPmF",
        "colab_type": "text"
      },
      "source": [
        "# <a id=\"sec7\"></a>Going further\n",
        "\n",
        "A lot of contributions have built on the initial success of DQN. Among those, some are combined and discussed in the **[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)** paper. We will simply summarize their key ideas here, by decreasing order of importance (according to the paper).\n",
        "\n",
        "- N-step returns. Use samples that accumulate several returns rather than the 1-step return of TD(0).\n",
        "- [Prioritized experience replay](https://arxiv.org/abs/1511.05952). Inspired by the model-based Prioritized Sweeping approach, bias the distribution used to sample mini-batches in order to present high residual samples to the optimizer. This accelerates the convergence in $L_\\infty$ norm.\n",
        "- [Distributional value functions](https://arxiv.org/abs/1707.06887). Instead of estimating $\\mathbb{E}(\\sum_t r_t)$, estimate the distribution of $\\sum_t r_t$ and iterate on it.\n",
        "- [NoisyNet](https://arxiv.org/abs/1706.10295). Instead of an $\\epsilon$-greedy exploration strategy, introduce noise in the network's parameters to drive the exploration.\n",
        "- [Dueling architecture](https://arxiv.org/abs/1511.06581). The neural network's architecture splits $Q$ into the estimation of a value $V(s)$ and an advantage $A(s,a)$ with shared first layers.\n",
        "- [Double Q-learning](https://arxiv.org/abs/1509.06461). Q-learning is prone to over-estimation of the true optimal Q function (especially in high variance environments). Double Q-learning aims at compensating this weakness by introducing an under-estimation mechanism based on a second Q function.\n",
        "\n",
        "Beyond these improvements, new work is published each year that lead to better understanding of the interplay between Deep Learning and RL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeyR3eOQvPmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}